{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset,mnist_info=tfds.load(name='mnist',with_info=True,as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " 'train': <PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train=mnist_dataset['train']\n",
    "mnist_test=mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples,tf.int64)\n",
    "num_test_samples=mnist_info.splits['train'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples,tf.int64)\n",
    "\n",
    "def scale(image,label):\n",
    "    image=tf.cast(image,tf.float32)\n",
    "    image=image/255.#dot. we want the reult to be in float\n",
    "    return image,label\n",
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "\n",
    "test_data=mnist_test.map(scale)\n",
    "\n",
    "\n",
    "#shuffling , so there is no oredred data\n",
    "#when we are deling wwith enormous datasets, we cant shyffle all data at once.\n",
    "\n",
    "BUFFER_SIZE=10000# in this way we incerase computional power\n",
    "\n",
    "#Shuffle at a batch of 10000,if buffer size=1, no shuffling will happen\n",
    "\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "#ACTUALLY EXTRACT TRAIN AND VALIDATION DATA\n",
    "\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)#to extract given samples samples\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)#skip first entered samples\n",
    "\n",
    "#we will use many batches,have trade off between speed and accuracy \n",
    "#batch size=1==>SGD\n",
    "#batch size =#samples==>single batch ,GD\n",
    "#1<batch size<#samples==>mini batch GD==> we will use this\n",
    "\n",
    "\n",
    "BATCH_SIZE=100\n",
    "\n",
    "#dataset.batch(batch_size)=>a method that combines the consecutve elements of a datset into batches\n",
    "\n",
    "train_data=train_data.batch(BATCH_SIZE)#IT WILL ADD NEW COLUMN TO OUR TENSOR\n",
    "#THAT WILL INDICATE HOW MANY SAMPLES SHOULD WE TAKE IN EACH BATCH\n",
    "\n",
    "# we dont need to batch on validation data,as we dont need to back prpogate onn validation data but only forwrd propgating\n",
    "#batching was used  when updating weights only once per batch i.e. 100 samples rather than on every samples.hence reducing\n",
    "#noise in traning updates. when batching we find the averge loss and average accuracy.during  validation and testing we want \n",
    "#exact values  so taking all data once.while forward propagating  we dont neeed much computational power so no batching required.\n",
    "#however the model expects the validation datset inn batch form too that why we overwrite validation data\n",
    "\n",
    "validation_data=validation_data.batch(num_validation_samples)#here we will have single batch with batch size=total number of samples\n",
    "#all data size at once\n",
    "\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs,validation_targets=next(iter(validation_data))\n",
    "#iter() creates  an object which can be iterated one element at a time (e.g. in a for loop or while loop),pythonic\n",
    "#synatx to make validation_data as an iterable\n",
    "#next() loads the next element of an iteratble object in this case it is batch, as there is one batch it will load inputs and \n",
    "#targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outline the model\n",
    "#input layer=784,2 hidden layer with units 50 each,output layer=10\n",
    "#try fine tune hyperparametes that is width and depth\n",
    "input_size=784\n",
    "output_size=10\n",
    "hidden_layer_size=50#assuming each hidden layer has same no of units\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),#relu=rectified linear unit\n",
    "    tf.keras.layers.Dense(output_size,activation='softmax')#it gives the probability of outputs\n",
    "    \n",
    "    \n",
    "    \n",
    "])\n",
    "#our data is (from tfds) is such that each input is 28*28*1 or tensor of rank 3\n",
    "\n",
    "#tf.keras.layers.Flatten(orignal shape)=>transforms or flattens a tensor into a vector\n",
    "#we need to flatten the images into vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opyimizer and loss function\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "#Binary_crossentropy->where we have got binary coding,categorical_crossentropy->\n",
    "#expects that we have one hot encoded the targets\n",
    "#sparse_categorical_crossentropy-->applies one hot encoding\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 20s - loss: 0.4145 - accuracy: 0.8822 - val_loss: 0.2343 - val_accuracy: 0.9335\n",
      "Epoch 2/5\n",
      "540/540 - 15s - loss: 0.1846 - accuracy: 0.9465 - val_loss: 0.1768 - val_accuracy: 0.9463\n",
      "Epoch 3/5\n",
      "540/540 - 17s - loss: 0.1393 - accuracy: 0.9593 - val_loss: 0.1434 - val_accuracy: 0.9562\n",
      "Epoch 4/5\n",
      "540/540 - 15s - loss: 0.1147 - accuracy: 0.9664 - val_loss: 0.1206 - val_accuracy: 0.9650\n",
      "Epoch 5/5\n",
      "540/540 - 14s - loss: 0.0976 - accuracy: 0.9710 - val_loss: 0.1102 - val_accuracy: 0.9655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fb96525808>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "training\n",
    "steps\n",
    "1>at the begining of each epoch, the training loss will be set to 0.\n",
    "2>the algorithm will iterate over a present number of batches,all from train data.\n",
    "3> the weights and bises will be updated as many times as there are batches.\n",
    "4> we will get a value for the loss function, indicating how the training is going.\n",
    "5>we will aslo see a tarining accuracy.\n",
    "6> at the end of the epoch, the algo will forward propagate the whole  validation set\n",
    "*when we reach the maximum number of epochs the training  will be over\n",
    "\n",
    "the accuracy shows in what percent of the cases our outputs were equal to the targets\n",
    "we keep a eye over validation loss (or set early stopping machanics) to determine whether the model is overfittig\n",
    "val_accuracy=true accuracy of model\n",
    "train accuracy is accuracy of batch.\n",
    "\n",
    "The error happens because a tf.Dataset is provided to the argument validation_data of Model.fit,\n",
    "but Keras does not know how many steps to validate for.\n",
    "To solve this problem, you can just set the argument validation_steps. For example:\n",
    "\"\"\"\n",
    "\n",
    "NUM_EPOCHS=5\n",
    "\n",
    "model.fit(train_data,epochs=NUM_EPOCHS,validation_data=(validation_inputs,validation_targets),verbose=2,validation_steps=num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 2s 2s/step - loss: 0.1180 - accuracy: 0.9656\n",
      "test loss== 0.11797527968883514\n",
      "test accuracy== 96.56000137329102\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "test the model\n",
    "play with various hyperparameteres,\n",
    "crate a model--> fiddle with hyperparameters--> check validation acuracy\n",
    "we might overfit validation dataset\n",
    "so test dataset as a reality check\n",
    "getting a test accuracy close to the validation set accuracy shows that we have not overfit\n",
    "\"\"\"\n",
    "test_loss,test_accuracy=model.evaluate(test_data)\n",
    "print('\\ntest loss==',test_loss)\n",
    "print('test accuracy==',test_accuracy*100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-dbf554a4678a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"group\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<<\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0m_check_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m       \u001b[0mend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_check_index\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m    750\u001b[0m     \u001b[1;31m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;31m# will break `_slice_helper` contract.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_SLICE_TYPE_ERROR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", got {!r}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'group'"
     ]
    }
   ],
   "source": [
    "for item in test_data:\n",
    "    xi, yi = item\n",
    "    pi = model.predict_on_batch(xi)\n",
    "    print(xi[\"group\"].shape, pi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
